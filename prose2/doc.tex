\documentclass[12pt,a4paper]{article}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}

\usepackage[english]{babel}
\usepackage{csquotes}

\usepackage[authordate-trad,backend=biber]{biblatex-chicago}
\addbibresource{jabref.bib}

%opening
\title{By how far have recent churn-reduction techniques improved quality of experience in peer-to-peer video streaming environments?}
\author{Martin Symons}

\begin{document}

\maketitle

\begin{abstract}
abstract !!! 
\end{abstract}

\section{Introduction}
Within the past ten years, video streaming traffic across the internet has exploded. Traditional client-server architectures places a large load on the small portion of nodes controlled by the streaming host, and lead to dramatic infrastructure costs. To combat this, peer-to-peer systems have been proposed that spread usage across the network's collective resources. Starting with Coolstreaming in 2004, these networks became defacto for IPTV streaming, especially pirated streams. With their utility proven, research accelerated, in pace and complexity. Industry, however, did not pick up. Since the shutdown of New Coolstreaming and PPLive in the late 2000's, these newer solutions have seen little real-world usage.

As such, the actual impact of current research on network performance is vague. Still, research since has suggested that churn has a substantial and exponential impact on client quality-of-service (QoS) beyond that which was considered in these original models. This paper investigates several marked improvements over the best-documented benchmark architecture, New Coolstreaming, with particular focus on methods to reduce churn throughout the network. wordsw words words 

\section{Literature Review}
\subsection{Peer-to-Peer Streaming Fundamentals}
In the late 1990's the traditional client-server architecture began to crack, as computer capabilities began to outpace bandwidth growth across the internet. Small clusters of server nodes were expected to take the burden, whilst ample client resources waited idle. BitTorrent allows a swarm of peers to collaborate in file transmission, maximizing global utilization without overwhelming any single node. Peers instead connect to a tracker or query a global DHT, usually \textit{Mainline DHT}, to gather a list of participatory nodes. Targetting the rarest blocks first, data chunks within the swarm are piped directly over UDP node-to-node. As more peers join the network, the overall bandwidth increases, with download speeds following suit.

This approach exploded onto the internet, claiming 35\% of all upstream traffic (\cite{archiveCacheLogicResearch}). It remained in the number one spot for total traffic until recently - finally dethroned in 2024 by video streaming. 

In a file-share network QoS is taken mostly with upload speed, and by extension bandwidth utilization. Any intermediate action from loading a \textit{.torrent} to completion is irrelevant. Video streaming, in contrast, introduces a large number of real-time requirements essential for a good user experience.



Peer-to-peer networks pipe data directly between nodes through a network, contrasted with the traditional client-server architecture seen most often today.  Compared to file-sharing applications, peer-to-peer video networks face several new complexities. In the former, QoS is ruled almost entirely by bandwidth utilization. Users care only that a desired file eventually reaches them; the journey it takes to completion is unimportant.  probably list some examples Video livestreaming, however, introduces new complexities. playoutdeadlines startupdelay end-to-enddelay
one proposed solution was ip multicasting. it was . fucked. s owe don't use it and instead use the various better solutions
\subsection{Performance Heuristics and Churn}
explaining why we use/care about churn vs other characteristics
\subsection{Overlay Structure}
\subsection{Peer Selection Strategies}
\subsection{Chunk Schedulers}
\subsection{Historic Implementations}
We now consider historic implementations of peer-to-peer streaming systems in relation to the above characteristics. Of most relevance to our paper is DONet, commercially branded Coolstreaming (\cite{1498486}). Considered the first of its breed, Coolstreaming \cite{Xie2007} \cite{asdasd} \cite{1498486}
\subsection{Implementation-Specific Improvements}
\section{Methodology}
our intended methodology. this is essentially just "we were going to implement coolstreaming, then a few improvements, then a monolithic solution for comparison" with maybe some dates and whatever
\section{Implementation}
discussing the failure of coolstreaming and what that involves
\section{Results}
jsut offhandedly mentioning that Yes, It Workas Kind Of
\section{Discussion}
\subsection{The Coolstreaming Family Tree}
looking at the various names and confusion this has caused, in this project and others
\subsection{The Fully-Connected Network Problem}
the difficulty in specifying a partnership protocol that can handle networks of M+1. this is no longer about it being impossible !! simply that this is non-trivial and should have been specified
\subsection{Choosing New Neighbours}
disabling a neighbour inequality to force the network to funciotn. fuck off
\subsection{Requesting the Block}
lack of clarity in playout index calculations. we already know this sucks
\subsection{Brainslugging and Production Optimization}
the fucked up implementation of scamp and other production optimizations
\subsection{What does Coolstreaming need to be?}
a research network. nobody cares about this thing for absolute production performance anymore, we are far far beyond that point. it is more important to operate as a baseline.
\subsection{Introducing \textit{coolstreaming-spiked}}
our proposals for a better coolstreaming for the research community. unfucked scamp, the partnerlink specified partnership protocol, and some other things that we clearly do not have answers for [if we did, we'd have a working network!] but should be Open Research Questions
\subsection{The \textit{Partnerlink} algorithm}
simple definition of partnerlink. remember we need this because of the research network's shape - we want to keep the switching algorithm, and this was already a problem within donet, now worsened in new coolstreaming.
\section{Conclusion}
holy fuck. this sucks

\printbibliography
\end{document}
